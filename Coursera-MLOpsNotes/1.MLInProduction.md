# Machine Learning In Production
- [ML Lifecycle](#mllifecycle)
- ML Project [Deployment Patterns](#deploypatterns)
- ML Project [Cycles - Building and After Deployment](#cycles)
- ML project [Drift - After Deployment](#drift) 
- ML project [Metrics - In Production](#metrics) 

## MLLifeCycle
[ML LifeCycle Diagram](./MLLifeCycle.png) </br>
[LandingAI_Platform_WhitePaper](https://landing.ai/wp-content/uploads/2021/06/LandingAI_Platform_WhitePaper.pdf) </br>
- Identify the key components 
  - ML System = Code + Model (algo + param + traindata features) + Data 
  - Ownership of code /data in time and place changes. Service does not own customer data.
    - [ ] Scope the Project
       - f(x) = y 
    - [ ] Get Data 
      - Acquire x
      - Labeling y
    - [ ] **Build Model** 
      - *Iterative Process* (Tune, train data, params, model)  
      - Build - f(x) - Research find the right architecture
      - Hyper Parameters 
      - Train
      - Error Analysis 
        - Classification, Regression, Generation
    - [ ] Deploy 
      - *Iterative Process* (Tune - metrics monitored, percentage served with model) - This is the BEGINNING
      - Test Deployed
      - Deploy
    - [ ] Monitor And Maintain the System
      - gather production data, monitor data, analyze 
      - **Update the Model** continuously - Production - Architecture/Algorithm is fixed, Data HyperParameters change 

## Cycles 
- Compare and contrast the ML modeling iterative cycle with the cycle for deployment of ML products.
  - Build and update data based on Error Analysis
    
## DeployPatterns
- Deployment scenarios in the context of varying degrees of automation.
  - shadow 
      - when there is a human doing the same task
      - deploy in parallel to human task to capture the labeled data and validate with human label.
  - canary
      - route 5% request to the model and rest to human
      - ensure the system/model works before switching ot use for more requests 
  - blue-green
      - When deploying V2(green), keep the V1)(blue) in deployment environment to revert immediately.
  
## Drift
[Reference: Concept Data Drift](https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb)
  - Data Drift - change in data distribution
      - Data drift, feature drift, population, or covariate shift. 
      - Mean and Variance shift of data columns.
  - Concept Drift - change in data relationship 
      - Model Decay
      - Incoming data varies significantly from trained data
      - Completely new data as opposed to diff data as in data drift 
  - Example/Reference Tools:
      - [OpenSource Model and Data Monitoring tools](https://github.com/evidentlyai/evidently)

## Metrics
- Monitoring the deployed system </br>
[Reference: Data/Concept(Model) Drift](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/) </br>
[Reference: Data vs Model Centric in production](https://www.youtube.com/watch?v=06-AZXmwHjo)  </br>
  - Dashboard - List the typical metrics you might track to monitor concept drift.
  - Software Metrics - Server Load  
      - Memory
      - CPU
      - Latency
      - Through put
      - Availability
      - Reliability
  - Model Metrics
    - Input Data 
      - Structured Data
          - Non null outputs
          - number of in-coming columns
      - UnStructured Data - thresholds
          - Image brightness, size, resolution
          - Audio . video stream length
          - How much of clip processed is empty
    - Output Data
      - User Exceptional processing usage when model did not help user
